# -*- coding: utf-8 -*-
"""Midterm Project (YOLO): Implementing Object Detection on a Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12UKr9-KFPhVSGmq8mjfO1iyzrkLDnj5G

# Midterm Project (YOLO): Implementing Object Detection on a Dataset

## Selection of Dataset and Algorithm
* Each student will choose a dataset suitable for object detection tasks. The dataset can be from publicly available sources **(e.g., COCO, PASCAL VOC)** or one they create.
* Select an object detection algorithm to apply to the chosen dataset. Possible algorithms include:
  * **YOLO (You Only Look Once)**: A real-time deep learning-based approach.
  * **SSD (Single Shot MultiBox Detector)**: A deep learning method balancing speed and accuracy.

### Dataset Information

The dataset utilized for this midterm activity is the Oxford-IIIT Pet Dataset, developed by the Visual Geometry Group at Oxford. This dataset consists of 37 categories of pet images, with approximately 200 images per class. The images exhibit significant variations in scale, pose, and lighting conditions. Each image is accompanied by detailed annotations, including breed classification, head region of interest (ROI), and pixel-level trimap segmentation.

The dataset includes 12 cat breeds and 25 dog breeds, listed as follows:

**Cat Breeds**:
- Abyssinian
- Bengal
- Birman
- Bombay
- British Shorthair
- Egyptian Mau
- Maine Coon
- Persian
- Ragdoll
- Russian Blue
- Siamese
- Sphynx

**Dog Breeds**:
- American Bulldog
- American Pit Bull Terrier
- Basset Hound
- Beagle
- Boxer
- Chihuahua
- English Cocker Spaniel
- English Setter
- German Shorthaired
- Great Pyrenees
- Havanese
- Japanese Chin
- Keeshond
- Leonberger
- Miniature Pinscher
- Newfoundland
- Pomeranian
- Pug
- Saint Bernard
- Samoyed
- Scottish Terrier
- Shiba Inu
- Staffordshire Bull Terrier
- Wheaten Terrier
- Yorkshire Terrier

Parkhi, O. M., Vedaldi, A., Zisserman, A., & Jawahar, C. V. (2012). Cats and dogs. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. Institute of Electrical and Electronics Engineers.

## Implementation

### Data Preparation

- Preprocess the dataset by resizing images, normalizing pixel values, and, if necessary, labeling bounding boxes for objects.
"""

# Import the Roboflow library for interacting with the API
from roboflow import Roboflow

rf = Roboflow(api_key="TUXckiOYdWfPKbzjriHg")
project = rf.workspace("feature-extraction-p1jos").project("cat-dog-breeds")
version = project.version(1)
dataset = version.download("yolov9")

"""### Model Building

- Implement the selected object detection algorithm using appropriate libraries.
"""

# Import the YOLO class from the ultralytics library to utilize YOLO object detection capabilities
from ultralytics import YOLO

# Load a pre-trained YOLO model using the specified weight file ('yolov9m.pt')
yolo_model = YOLO('yolov9m.pt')

"""### Training the Model

- Use the training data to train the object detection model. For deep learning methods, fine-tune hyperparameters (e.g., learning rate, batch size, epochs) to optimize model performance.
"""

# Train the YOLO model using the specified parameters
yolo_model.train(
    data="/content/Cat-&-Dog-Breeds-1/data.yaml", # Path to the dataset configuration file (defines classes and dataset splits)
    epochs=25,                                    # Number of training epochs (iterations over the entire dataset)
    batch=16,                                     # Batch size for training (number of images processed simultaneously)
    imgsz=640,                                    # Input image size (dimensions for resizing images before training)
    lr0=0.01,                                     # Initial learning rate for the optimizer
)

"""### Testing

- Evaluate the model on a test set to assess its detection capabilities. Ensure to capture edge cases where the model may struggle.
"""

from google.colab import files
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
import torch
import cv2
import numpy as np
import io
import time
from collections import defaultdict

# Constants for visualization
BOX_THICKNESS = 3        # Thickness of the bounding box around detected objects
FONT_SIZE = 72           # Font size for labels
TEXT_PADDING = 8         # Padding for text background
BOX_COLOR = (255, 0, 0)  # Color of bounding boxes (red in RGB)
TEXT_BG_ALPHA = 230      # Transparency level of the text background

def upload_images():
    """
    Prompts the user to upload exactly 6 images via Google Colab's file uploader.

    Returns:
        dict: A dictionary containing image names as keys and file data as values.
    """
    uploaded_images = {}
    while len(uploaded_images) < 6:
        print(f"Please upload {6 - len(uploaded_images)} more image(s).")
        uploaded = files.upload()
        uploaded_images.update(uploaded)

    if len(uploaded_images) > 6:
        print("Too many images uploaded. Only the first 6 will be used.")
        uploaded_images = dict(list(uploaded_images.items())[:6])

    print("All 6 images uploaded successfully.")
    return uploaded_images

def crop_to_square(image):
    """
    Crops an image to a 1:1 aspect ratio by taking a centered square region.

    Args:
        image (PIL.Image.Image): The input image.

    Returns:
        PIL.Image.Image: The cropped image.
    """
    width, height = image.size
    size = min(width, height)
    left = (width - size) // 2
    top = (height - size) // 2
    right = left + size
    bottom = top + size
    return image.crop((left, top, right, bottom))

def calculate_metrics(results, detection_time, image_name):
    """
    Computes performance metrics for the object detection results.

    Args:
        results (object): YOLO detection results containing bounding box data.
        detection_time (float): Time taken for detection in seconds.
        image_name (str): Name of the processed image.

    Returns:
        dict: A dictionary of metrics including confidence levels, detection counts,
              and class distribution.
    """
    confidence_threshold = 0.5  # Minimum confidence threshold for high-confidence detections
    metrics = {
        'image_name': image_name,
        'total_detections': len(results.boxes.data),
        'high_confidence_detections': sum(1 for det in results.boxes.data if det[4] > confidence_threshold),
        'avg_confidence': 0,
        'detection_time': detection_time,
        'class_distribution': defaultdict(int)
    }

    if len(results.boxes.data) > 0:
        confidences = [det[4].item() for det in results.boxes.data]
        metrics['avg_confidence'] = sum(confidences) / len(confidences)

        # Calculate class distribution
        for det in results.boxes.data:
            class_name = results.names[int(det[5])]
            metrics['class_distribution'][class_name] += 1

    return metrics

def process_single_image(image_bytes, yolo_model, image_name):
    """
    Processes a single image for object detection, visualizes results, and calculates metrics.

    Args:
        image_bytes (bytes): Image data in bytes format.
        yolo_model (torch.nn.Module): Trained YOLO model for object detection.
        image_name (str): Name of the image being processed.

    Returns:
        tuple: A tuple containing the processed image (PIL.Image.Image) and detection metrics (dict).
    """
    # Load and preprocess the image
    image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
    img_array = np.array(image)

    # Perform object detection and measure time
    start_time = time.time()
    results = yolo_model(img_array)[0]
    detection_time = time.time() - start_time

    # Compute metrics for the detections
    metrics = calculate_metrics(results, detection_time, image_name)

    # Draw bounding boxes and labels
    draw_image = image.convert('RGBA')
    draw = ImageDraw.Draw(draw_image)
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", FONT_SIZE)
    except:
        font = ImageFont.load_default()

    for det in results.boxes.data:
        x1, y1, x2, y2, conf, cls = map(float, det)
        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])

        # Draw bounding box
        for i in range(BOX_THICKNESS):
            draw.rectangle([x1+i, y1+i, x2-i, y2-i], outline=BOX_COLOR)

        # Label with class name and confidence
        class_name = results.names[int(cls)]
        label_text = f"{class_name} - {conf*100:.1f}%"
        text_bbox = draw.textbbox((0, 0), label_text, font=font)
        text_width = text_bbox[2] - text_bbox[0]
        text_height = text_bbox[3] - text_bbox[1]

        bg_x1 = x1
        bg_y1 = max(0, y1 - text_height - TEXT_PADDING * 2)
        bg_x2 = x1 + text_width + TEXT_PADDING * 2
        bg_y2 = y1

        overlay = Image.new('RGBA', draw_image.size, (0, 0, 0, 0))
        overlay_draw = ImageDraw.Draw(overlay)
        overlay_draw.rectangle([bg_x1, bg_y1, bg_x2, bg_y2], fill=(*BOX_COLOR, TEXT_BG_ALPHA))

        draw_image = Image.alpha_composite(draw_image, overlay)
        draw = ImageDraw.Draw(draw_image)
        draw.text((bg_x1 + TEXT_PADDING, bg_y1 + TEXT_PADDING), label_text, fill="white", font=font)

    processed_image = crop_to_square(draw_image.convert('RGB'))
    return processed_image, metrics

def display_grid(images):
    """
    Displays a list of images in a 3x2 grid layout using Matplotlib.

    Args:
        images (list): List of PIL.Image.Image objects to display.
    """
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    plt.subplots_adjust(wspace=0.1, hspace=0.1)

    for ax, img in zip(axes.flat, images):
        ax.imshow(img)
        ax.axis('off')

    plt.tight_layout()
    plt.show()

def print_metrics(metrics):
    """
    Prints detection metrics in a readable format.

    Args:
        metrics (dict): Dictionary containing detection metrics.
    """
    print("\nPerformance Metrics:")
    print("-------------------")
    print(f"Image Name: {metrics['image_name']}")
    print(f"Total Detections: {metrics['total_detections']}")
    print(f"High Confidence Detections (>50%): {metrics['high_confidence_detections']}")
    print(f"Average Confidence: {metrics['avg_confidence']*100:.2f}%")
    print(f"Detection Time: {metrics['detection_time']*1000:.2f}ms")

    print("\nClass Distribution:")
    print("-" * 20)
    for cls, count in metrics['class_distribution'].items():
        print(f"{cls}: {count}")

def process_images(yolo_model):
    """
    The main pipeline for processing images with YOLO.

    Steps:
        1. Upload images.
        2. Perform detection on each image.
        3. Display results in a grid.
        4. Print detailed metrics.

    Args:
        yolo_model (torch.nn.Module): Trained YOLO model for object detection.
    """
    uploaded_images = upload_images()
    processed_images = []
    metrics_list = []

    for filename, image_bytes in uploaded_images.items():
        processed_image, metrics = process_single_image(image_bytes, yolo_model, filename)
        processed_images.append(processed_image)
        metrics_list.append(metrics)

    display_grid(processed_images)

    for metrics in metrics_list:
        print_metrics(metrics)

# Main execution
if __name__ == "__main__":
    process_images(yolo_model)

"""## Evaluation

* **Performance Metrics**: Assess the model's performance using various metrics, including:
  * **Accuracy**: Overall success rate of object detection.
  * **Precision**: The proportion of true positive detections out of all positive predictions.
  * **Recall**: The proportion of true positive detections out of all actual positives in the dataset.
  * **Speed**: Measure the time taken for the model to detect objects in an image or video frame.
* **Comparison**: Compare the results of the chosen model against other potential algorithms (e.g., how HOG-SVM compares to YOLO or SSD in terms of speed and accuracy).
"""

import numpy as np

def safe_extract_metrics(metrics, key):
    """
    Safely extract metrics, ensuring type conversion and error handling.

    Args:
    - metrics (list): List of dictionaries containing class-specific metrics.
    - key (str): The key corresponding to the metric to extract.

    Returns:
    - list: A list of extracted metric values as floats, with default values of 0.0 for errors or missing keys.
    """
    try:
        # Extract values and convert them to float, using 0.0 if the value is missing or invalid.
        return [float(metric[key]) if metric[key] is not None else 0.0 for metric in metrics]
    except (TypeError, ValueError):
        print(f"Error extracting {key} metrics. Defaulting to 0.0.")
        return [0.0] * len(metrics)

def extract_performance_metrics(results):
    """
    Extract comprehensive performance metrics from YOLO validation results.

    Args:
    - results: Validation results obtained from a YOLO model.

    Returns:
    - dict: A dictionary containing overall, speed, and class-specific performance metrics.
    """
    try:
        # Extract speed-related metrics with a fallback to an empty dictionary.
        speed_metrics = getattr(results, 'speed', {})

        # Initialize the metrics dictionary with overall performance values.
        metrics = {
            'mean_precision': getattr(results.box, 'mp', 0),  # Mean precision across all classes.
            'mean_recall': getattr(results.box, 'mr', 0),     # Mean recall across all classes.
            'map50': getattr(results.box, 'map50', 0),        # Mean average precision at IoU threshold 0.50.
            'map75': getattr(results.box, 'map75', 0),        # Mean average precision at IoU threshold 0.75.
            'map': getattr(results.box, 'map', 0),            # Overall mean average precision.
            'inference_speed': {                              # Time breakdown for inference stages.
                'preprocess': speed_metrics.get('preprocess', 0),
                'inference': speed_metrics.get('inference', 0),
                'postprocess': speed_metrics.get('postprocess', 0),
            },
            'class_metrics': []  # List to store class-specific performance metrics.
        }

        # Extract metrics for each class, handling potential errors per class.
        num_classes = getattr(results.box, 'nc', 0)
        for i in range(num_classes):
            try:
                precision, recall, ap50, ap = results.box.class_result(i)
                metrics['class_metrics'].append({
                    'class_index': i,
                    'precision': precision,
                    'recall': recall,
                    'ap50': ap50,
                    'ap': ap
                })
            except Exception as class_err:
                print(f"Error extracting metrics for class {i}: {class_err}")
                # Default values for metrics if extraction fails.
                metrics['class_metrics'].append({
                    'class_index': i,
                    'precision': 0,
                    'recall': 0,
                    'ap50': 0,
                    'ap': 0
                })

        return metrics

    except Exception as e:
        print(f"Error extracting performance metrics: {e}")
        return None

def print_detailed_performance(metrics, class_names):
    """
    Print a detailed breakdown of performance metrics for overall and class-specific performance.

    Args:
    - metrics (dict): Dictionary containing extracted performance metrics.
    - class_names (list): List of class names corresponding to the model's detected classes.
    """
    print("\n--- Overall Model Performance ---")
    print(f"Mean Precision: {metrics['mean_precision']:.4f}")
    print(f"Mean Recall: {metrics['mean_recall']:.4f}")
    print(f"mAP@0.50: {metrics['map50']:.4f}")
    print(f"mAP@0.75: {metrics['map75']:.4f}")
    print(f"Mean Average Precision: {metrics['map']:.4f}")

    print("\n--- Inference Speed ---")
    print(f"Preprocess Time: {metrics['inference_speed']['preprocess']:.2f} ms")
    print(f"Inference Time: {metrics['inference_speed']['inference']:.2f} ms")
    print(f"Postprocess Time: {metrics['inference_speed']['postprocess']:.2f} ms")

    print("\n--- Class-wise Performance ---")
    for i, metric in enumerate(metrics['class_metrics']):
        print(f"\n{class_names[i]}:")
        print(f"  Precision: {metric['precision']:.4f}")
        print(f"  Recall: {metric['recall']:.4f}")
        print(f"  AP@0.50: {metric['ap50']:.4f}")
        print(f"  AP: {metric['ap']:.4f}")

def enhance_model_evaluation(yolo_model, validation_data):
    """
    Perform a complete evaluation of the YOLO model using validation data.

    Args:
    - yolo_model: The trained YOLO model to evaluate.
    - validation_data (str): Path to the validation dataset in YAML format.

    Returns:
    - dict: A dictionary containing extracted performance metrics.
    """
    # Run validation on the dataset and retrieve results.
    results = yolo_model.val(data=validation_data)

    # Extract and structure the performance metrics.
    metrics = extract_performance_metrics(results)

    if metrics:
        # Print a detailed report of the performance metrics.
        print_detailed_performance(metrics, yolo_model.names)

    return metrics

# Usage example in the main script.
metrics = enhance_model_evaluation(yolo_model, "/content/Cat-&-Dog-Breeds-1/data.yaml")